# ğŸ§  Time-LLM â€” Unified Transformer + LLM Framework for Time-Series Forecasting

## ğŸš€ Overview

`Time-LLM` is a **modern re-implementation and extension** of classical temporal forecasting architectures such as **Autoformer** and **DLinear**, augmented with a **language-modelâ€“aware reasoning module**.
This repository is designed for:

* ğŸ”¬ **ML/AI research** in temporal modeling
* ğŸ§© **MLOps deployment experiments**
* ğŸ’¼ **Industry-grade forecasting pipelines** (energy, finance, retail, etc.)

Unlike existing public repos (e.g., *KimMeen/Time-LLM*), this version emphasizes:

| Area                        | Enhancement                                                                     |
| --------------------------- | ------------------------------------------------------------------------------- |
| âš™ï¸ **Code Reliability**     | Cross-platform (macOS / MPS / CPU / CUDA) compatible, device-safe, reproducible |
| ğŸ“ˆ **Engineering Quality**  | Accelerate + AMP integration, early-stopping, scheduler tuning, modular config  |
| ğŸ§  **Research Depth**       | Supports Autoformer, DLinear, and LLM-based hybrid architectures                |
| ğŸŒ **Deployment Readiness** | Designed for Streamlit / FastAPI serving; logging via TensorBoard               |
| ğŸ”„ **Originality**          | Clean, re-structured `run_main.py` with advanced fallback and error-handling    |

---

## ğŸ§© Architecture

```text
+----------------------------------------------------+
|                  Data Pipeline                     |
| Loader â†’ Normalizer â†’ Sequence Windowing           |
+----------------------------------------------------+
                         |
                         v
+----------------------------------------------------+
| Temporal Encoder (Autoformer / DLinear)            |
| - Captures seasonal-trend decomposition            |
| - Handles long-term temporal dependencies          |
+----------------------------------------------------+
                         |
                         v
+----------------------------------------------------+
| LLM-Infused Alignment Layer (Time-LLM)             |
| - Text-guided context alignment                    |
| - Cross-attention fusion with time embeddings      |
+----------------------------------------------------+
                         |
                         v
+----------------------------------------------------+
| Forecast Head + Evaluation Metrics                 |
| - Predicts future sequences (MSE / MAE / RMSE)     |
+----------------------------------------------------+
```

---

## ğŸ§ª Key Features

* ğŸ§  **Autoformer**, **DLinear**, and **Time-LLM** architectures
* âš¡ **Accelerate**-based distributed training (supports CPU, MPS, CUDA)
* ğŸ§© **Cross-attention fusion** between numeric and textual modalities
* ğŸ” **Early Stopping**, **OneCycle LR**, and **Cosine Annealing**
* ğŸ” **Explainability hooks** for attention visualization
* ğŸ’¾ **Checkpoint + Resume** compatible
* ğŸ§° **Completely device-agnostic** â€“ runs seamlessly on Mac Silicon, Windows, and Linux

---

## ğŸ§° Tech Stack

| Domain       | Libraries / Frameworks                    |
| :----------- | :---------------------------------------- |
| Core ML / DL | PyTorch 2.x â€¢ Accelerate â€¢ TorchMetrics   |
| Time Series  | Autoformer â€¢ DLinear â€¢ TimeLLM            |
| Utilities    | NumPy â€¢ Pandas â€¢ TQDM â€¢ Matplotlib        |
| Deployment   | Streamlit (optional) â€¢ FastAPI (optional) |
| DevOps       | Git â€¢ Shell â€¢ Python 3.11                 |

---

## ğŸ§­ Directory Structure

```text
Time-LLM/
â”‚
â”œâ”€â”€ models/                  # Core model architectures (Autoformer, DLinear, TimeLLM)
â”œâ”€â”€ layers/                  # Custom embeddings, attention, and transformer blocks
â”œâ”€â”€ data_provider/           # Data preprocessing, windowing, and normalization logic
â”œâ”€â”€ utils/                   # Helper utilities (early stopping, learning rate schedulers)
â”œâ”€â”€ dataset/                 # Local or downloaded datasets
â”œâ”€â”€ checkpoints/             # Saved model checkpoints during training
â”œâ”€â”€ run_main.py              # Main training script with Accelerate + AMP integration
â”œâ”€â”€ run_pretrain.py          # Optional pretraining script
â”œâ”€â”€ requirements.txt         # Required Python packages
â””â”€â”€ README.md                # Project documentation (you are here)
```

---

## âš™ï¸ Setup Instructions

```bash
# 1ï¸âƒ£ Clone
git clone https://github.com/RajxPatil/Time-series-forecasting-LLM.git
cd Time-series-forecasting-LLM

# 2ï¸âƒ£ Create virtual environment
python3 -m venv venv
source venv/bin/activate   # (or venv\Scripts\activate on Windows)

# 3ï¸âƒ£ Install dependencies
pip install -r requirements.txt

# 4ï¸âƒ£ Run training (CPU-safe default)
python run_main.py \
  --task_name long_term_forecast \
  --is_training 1 \
  --model Autoformer \
  --data ETTm1 \
  --root_path ./dataset \
  --data_path ETTm1.csv \
  --features S \
  --enc_in 1 --dec_in 1 --c_out 1 \
  --seq_len 96 --label_len 48 --pred_len 96 \
  --train_epochs 10 --batch_size 8 \
  --learning_rate 1e-4 \
  --num_workers 0 \
  --moving_avg 25 --factor 1 --dropout 0.05 \
  --embed timeF --activation relu --patience 3
```

> ğŸ’¡ *For macOS (M-series):* Add the following line in `run_main.py` (before training) to enable hybrid CPU/MPS fallback:
>
> ```python
> os.environ["PYTORCH_ENABLE_MPS_FALLBACK"] = "1"
> ```

---

## ğŸ“Š Example Results (ETTm1 Dataset)

| Metric                        |  Epoch 1 |  Epoch 2 |
| :---------------------------- | :------: | :------: |
| **Train Loss**                | 0.255873 | 0.161460 |
| **Validation Loss**           | 0.096348 | 0.083173 |
| **Test Loss**                 | 0.060799 | 0.048367 |
| **MAE (Mean Absolute Error)** | 0.190344 | 0.170858 |

> ğŸ’¡ *Results were obtained on macOS (CPU mode) with a batch size of 8, learning rate of 1e-4, and 2 training epochs.*

---

## ğŸ§© Original Contributions

1. ğŸ”§ **Device-Agnostic Training Pipeline**
   Seamless execution on CPU, CUDA, and Apple MPS with automatic fallback.

2. âš™ï¸ **Re-Engineered `run_main.py`**
   Clean modular structure, better exception handling, and AMP + Accelerate integration.

3. ğŸ§  **Enhanced Data Provider**
   Ensures type consistency, float32 precision, and efficient data loading.

4. ğŸª¶ **Improved Checkpoint Management**
   Automatic cleanup, resume support, and structured saving under experiment IDs.

5. ğŸ§© **LLM Integration Framework**
   Hooks designed to integrate GPT2 / Llama-style embeddings for semantic time-series reasoning.

6. ğŸ“Š **Readable, Modular, and Recruiter-Friendly Codebase**
   Designed with readability and engineering precision for portfolio-grade presentation.

---

## ğŸš€ Possible Extensions

* ğŸ”— **Integrate LLM Embeddings:** Add GPT2 or CodeBERT embeddings for semantic time-series alignment.
* â˜ï¸ **API Deployment:** Wrap inference in a FastAPI/Streamlit endpoint for real-time serving.
* ğŸ§® **Experiment Tracking:** Integrate with Weights & Biases (W&B) for reproducible experiment logs.
* ğŸ” **Explainability:** Implement attention heatmaps for interpretability and visualization.
* ğŸ§© **Multimodal Forecasting:** Extend to include external metadata (text/weather/news signals).

---

## ğŸ™Œ Acknowledgements

* ğŸ§© Original concept inspired by *Kim Meen et al., 2024 â€” Time-LLM: Time Series Forecasting via Large Language Models*.
* ğŸ”¬ Re-implemented and optimized independently by **Raj Patil** (IIT Patna).
* ğŸ’¡ Built using open-source frameworks: **PyTorch**, **Hugging Face Accelerate**, and **TorchMetrics**.
* â¤ï¸ Thanks to the open research community for enabling reproducibility and transparency.

---

## ğŸ§  Contact

**Raj Patil**
ğŸ“§ Email: [rajpatil172004@gmail.com](mailto:rajpatil172004@gmail.com)
ğŸŒ GitHub: [RajxPatil](https://github.com/RajxPatil)
ğŸ’¼ LinkedIn: [linkedin.com/in/rajxpatil](https://linkedin.com/in/rajxpatil)
ğŸ“ Indian Institute of Technology, Patna {B.Tech in Artificial Intelligence and Data Science (2022-2026)}